# -*- coding: utf-8 -*-
"""IndianLiver.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1egpjYtjh-WbEcNgGGhjHyZ6nIiVF8GVT
"""

import seaborn as sns
import pandas as pd
import numpy as np
import sklearn
from scipy import stats
import matplotlib.pyplot as plt
import os
import seaborn as sns
from sklearn.preprocessing import LabelEncoder 
from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split 
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
import tensorflow.keras
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense
from sklearn.ensemble import ExtraTreesClassifier
import joblib

#import the dataset from specified Location
data = pd.read_csv('/content/indian_liver_patient.csv')

#showing the data from top 5
data.head()

data.info()

data.isnull().any()

data.isnull().sum()

data['Albumin_and_Globulin_Ratio'].fillna(data['Albumin_and_Globulin_Ratio'].mode()[0], inplace=True)

lc = LabelEncoder() 
data['Gender'] = lc.fit_transform(data['Gender'])

data.describe()

sns.distplot(data['Age'])
plt.title('Age Distribution Graph')
plt.show()

sns.countplot(x=data['Albumin'],hue=data['Gender'])

plt.figure(figsize=(10,7))
sns.heatmap(data.corr(), annot=True)

# Define X as the input features in your dataset
X = data[['Age', 'Gender', 'Total_Bilirubin','Direct_Bilirubin', 'Alkaline_Phosphotase']]

# Scale the input features using the `scale` function from scikit-learn
x_scaled = pd.DataFrame(scale(X), columns=X.columns)

# Print the first few rows of the scaled data
print(x_scaled.head())

x_scaled.head()

X=data.iloc[:,:-1] 
y=data.Albumin

X_train, X_test, y_train, y_test = train_test_split(x_scaled,y, test_size=0.2, random_state=42)

smote = SMOTE()

y_train.value_counts()

X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)

smote = SMOTE()
print(y_train.value_counts())

"""# *Model Building* """

model1=RandomForestClassifier()
model1.fit(x_train_smote, y_train_smote)
y_predict=model1.predict(x_test)
rfc1-accuracy_score (y_test,y_predict)
rfc1
pd.crosstab (y_test, y_predict)
print(classification_report (y_test, y_predict))

model4=DecisionTreeClassifier()
model4.fit(x_train_smote, y_train_smote) 
y_predict=model4.predict(x_test)
dtc1=accuracy_score(y_test,y_predict) 
dtc1
pd.crosstab(y_test,y_predict)
print(classification_report(y_test, y_predict))

model2=KNeighborsClassifier()
model2.fit(X_train_smote, y_train_smote) 
y_predict = model2.predict(x_test)
knn1=(accuracy_score(y_test, y_predict)) 
knn1
pd.crosstab(y_test,y_predict)
print(classification_report(y_test, y_predict))

model5=LogisticRegression()
model5.fit(x_train_smote, y_train_smote) 
y_predict=model5.predict(x_test)
logi1=accuracy_score (y_test, y_predict)
logil
pd.crosstab(y_test,y_predict)
print (classification_report (y_test, y_predict))

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense (units=100, activation='relu', input_dim=10))

# Adding the second hidden Layer
classifier.add(Dense (units=50, activation='relu'))

# Adding the output layer
classifier.add(Dense (units=1, activation='sigmoid'))

# Compiling the ANN
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Fitting the ANN to the Training set
model_history = classifier.fit(X_train, y_train, batch_size=100, validation_split =0.2, epochs=100)

1 #Age Gender Total Bilrubin "Direct Bilrubin Alkaline Phosphotase 2 model4.predict([[50,1,1.2,0.8,150,70,80,7.2,3.4,0.8])

#Age Gender Total_Bilrubin "Direct_Bilrubin Alkaline Phosphotase 2 model1.predict([[50,1,1.2,0.8,150,70,80,7.2,3.4,0.8]) "Alanin_Aminotransferase Asparate_Aminotrans

1 #Age Gender Total_Bilrubin Direct_Bilrubin Alkaline Phosphotase- 2 model2.predict([[50,1,1.2,0.8,150,70,80,7.2,3.4,0.8]])

#Age Gender Total_Bilrubin Direct_Bilrubin Alkaline Phosphotase "Alanin_Aminotransferase Asparate_Aminotrans
mode15.predict([[42,0,1.2,0.8,240,70,80,7.2,3.4,0.8]]) <

classifier.save("liver.h5")
y_pred = classifier.predict(x_test)y_pred
y_pred = (y_pred > 0.5)
y_pred = ([[True], [True], [True], [True]])

def predict_exit(sample_value):



# Convert list to numpy array
 sample_value = np.array (sample_value)

# Reshape because sample_value contains only 1 record sample_value = sample_value.reshape(1, -1)
# Feature Scaling
sample_value = scale(sample_value)
return classifier.predict(sample_value)
#Age Gender Total_Bilrubin Direct_Bilrubin Alkaline Phosphotase
sample_value = [[50,1,1.2,0.8,150,70,80,7.2,3.4,0.8]]
if predict_exit (sample_value)>0.5: 4 print('Prediction: Liver Patient')


else:
print('Prediction: Healthy ')

acc_smote= [['KNN Classifier', knn1], ['RandomForestClassifier', rfc1],
              ['DecisionTreeClassifier', dtc1], ['Logistic Regression', logi1]]

Liverpatient_pred= pd.DataFrame(acc_smote, columns = ['classification models', 'accuracy_score'])
Liverpatient_pred

plt.figure(figsize=7,5)
plt.xticks(rotation=90)
plt.title('Classification models & accuracy scores after SMOTE', fontsize=18) 
sns.barplot(x="classification models", y="accuracy_score", data=Liverpatient_pred, palette ="Set2

model=ExtraTreesClassifier()
model.fit(x,y)

model.feature_importances_

dd=pd.DataFrame (model.feature_importances_, index-X.columns).sort_values(0, ascending=False) 
dd

dd.plot(kind='barh', figsize=(7,6)) 
plt.title("FEATURE IMPORTANCE", fontsize=14

joblib.dump(model1, 'ETC.pk1')

from flask import Flask, render_template, request
import numpy as np
import pickle